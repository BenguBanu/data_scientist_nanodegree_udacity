{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "## Dimensionality Reduction and PCA\n",
    "##### Elliot Partridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PCA?\n",
    "PCA (or principal component analysis) is a technique to reduce dimensionality. This technique is about taking your full dataset and reducing it to only the parts that hold the most information.\n",
    "\n",
    "### Latent Features\n",
    "Latent features are features that aren't explicitly in your dataset.\n",
    "\n",
    "Below are features that are all related to a latent feature, home size:\n",
    "\n",
    "- lot size\n",
    "- number of rooms\n",
    "- floor plan size\n",
    "- size of garage\n",
    "- number of bedrooms\n",
    "- number of bathrooms\n",
    "\n",
    "### Two Approaches : Feature Selection and Feature Extraction\n",
    "#### Feature Selection\n",
    "Feature Selection involves finding a subset of the original features of your data that you determine are most relevant and useful.\n",
    "\n",
    "##### Methods of Feature Selection:\n",
    "- **Filter methods** - Filtering approaches use a ranking or sorting algorithm to filter out those features that have less usefulness. Filter methods are based on discerning some inherent correlations among the feature data in unsupervised learning, or on correlations with the output variable in supervised settings. Filter methods are usually applied as a preprocessing step. Common tools for determining correlations in filter methods include: Pearson's Correlation, Linear Discriminant Analysis (LDA), and Analysis of Variance (ANOVA).\n",
    "\n",
    "- **Wrapper methods** - Wrapper approaches generally select features by directly testing their impact on the performance of a model. The idea is to \"wrap\" this procedure around your algorithm, repeatedly calling the algorithm using different subsets of features, and measuring the performance of each model. Cross-validation is used across these multiple tests. The features that produce the best models are selected. Clearly this is a computationally expensive approach for finding the best performing subset of features, since they have to make a number of calls to the learning algorithm. Common examples of wrapper methods are: Forward Search, Backward Search, and Recursive Feature Elimination.\n",
    "\n",
    "#### Feature Extraction\n",
    "Feature Extraction involves extracting, or constructing, new features called latent features.\n",
    "\n",
    "##### Methods of Feature Extraction\n",
    "Constructing latent features is the goal of Principal Component Analysis (PCA). Other methods for accomplishing Feature Extraction include Independent Component Analysis (ICA) and Random Projection.\n",
    "\n",
    "#### Principal Components\n",
    "An advantage of **Feature Extraction** over **Feature Selection** is that the latent features can be constructed to incorporate data from multiple features, and thus retain more information present in the various original inputs, than just losing that information by dropping many original inputs.\n",
    "\n",
    "Principal components are linear combinations of the original features in a dataset that aim to retain the most information in the original data.\n",
    "\n",
    "##### Principal Component Properties\n",
    "There are two main properties of principal components:\n",
    "\n",
    "1. They retain the most amount of information in the dataset.\n",
    "2. The created components are orthogonal to one another."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
