{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Introduction to Neural Networks\n",
    "##### Elliot Partridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Error Function:** Refers to how far away you are from the optimum solution. _(Error functions should not be discrete since otherwise you can get in a local basin)._\n",
    "- **Maximum Likelihood:** Picking the model that gives existing labels the highest probability.\n",
    "- **Cross Entropy:** The sum of the negative of the logarithm of the _(individual)_ probabilities is cross entropy. Low probability = high cross entropy. You should select the the model that produces the lowest cross entropy on known data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent \n",
    "\n",
    "- _\"The gradient is actually a scalar times the coordinates of the point! And what is the scalar? Nothing less than a multiple of the difference between the label and the prediction. What significance does this have... If a point is well classified, we will get a small gradient. And if it's poorly classified, the gradient will be quite large.\"_\n",
    "\n",
    "\n",
    "\n",
    "## Neural Networks \n",
    "\n",
    "### Architecture \n",
    "- **Input layer:** inputs to the model e.g. x1, x2 for 2 dimensional space, x1, x2, x3 for 3 dimensions etc.\n",
    "- **Hidden layer:** linear models created by these inputs. \n",
    "- **Output layer:** linear model combined to create a nonlinear model.\n",
    "\n",
    "If our output layer has multiple outputs then we have a multi-class classification model _e.g. a model predicting cat, dog, bird would have a an output score for each class._\n",
    "\n",
    "Having more layers in our hidden layers than our nodes means that we are combining the linear models in the hidden layer, creating nonlinear models and then combining these to create more nonlinear models \n",
    "\n",
    "## Feedforward \n",
    "\n",
    "_\"Feedforward is the process neural networks use to turn the input into an output.\"_\n",
    "\n",
    "## Backpropagation \n",
    "\n",
    "In a nutshell, backpropagation consists of:\n",
    "1. Doing a feedforward operation. \n",
    "2. Comparing the output of the model with the desired output. \n",
    "3. Calculating the error.\n",
    "4. Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights. \n",
    "5. Use this to update the weights, and get a better model.\n",
    "6. Continue this until we have a model that is good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
