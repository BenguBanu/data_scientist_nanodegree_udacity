{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Training Neural Networks\n",
    "##### Elliot Partridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Optimisation\n",
    "\n",
    "### Under/over Fitting\n",
    "\n",
    "- **Underfitting:** A.K.A not specific enough, this is when the model does not fit the data well. Sometimes this is reffered to as error due to bias \n",
    "- **Overfitting:** A.K.A too specific, this is when the model fits the data well but doesn't genralise. Sometimes this is reffered to as error due to variance\n",
    "\n",
    "Although one should always aspire to have a simple a model as possible if a model underfits then it is useless. It is better to have a more complex model that overfits than a model that does nothing at all since we can apply techniques that minimise the chances of overfitting.\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "- We should check when the model error on both the training & testing errors converge as low as they can be on a **model complexity graph**\n",
    "\n",
    "### Regularisation\n",
    "\n",
    "- To avoid overfitting where the model error is low we want to adjust the error function by punishing large coefficients adding a term that is big when model weights are big generally via a lambda function that either sums _(L1 regularisation)_ or sums the squared weights _(L2 regularisation)_. If we set the lambda to be large we punish large weights more\n",
    "\n",
    "#### Deciding between L1 or L2 regularisation:\n",
    "\n",
    "**L1:**\n",
    "- Tend to end up with sparse vectors _e.g. 1, 0, 1, 1_. This means that small weights will generally go to zero and therefore is useful for feature selection\n",
    "\n",
    "**L2:**\n",
    "- Tends not to favour spare vectors since it tries to maintain all weights  _e.g. 0.5, 0.3, -0.2, 0.11_. This usually gives better results for training models\n",
    "- The reason we get non-spare vectors with L2 can be explained by the following: \n",
    "$$ vector: (1,0) = sum & sqr of weights are always 1 but with vector: (0.5, 0.5)...$$\n",
    "$$ 1^2 + 0^2 = 1 $$ \n",
    "$$ 0.5^2 + 0.5^2 = 0.5 $$\n",
    "\n",
    "### Dropout\n",
    "\n",
    "- As we run through epochs randomly turning off certain nodes to make the other nodes take the slack and optimise. We do this by adding a probability that each node will get turn off in each epoch\n",
    "\n",
    "### Local Minima\n",
    "\n",
    "One way to get around local minima is to utilise **random restarts.** This means staring at many random places and completing gradient descent on them ensuring that we get to a pretty good overall minimum.\n",
    "\n",
    "Another way is to use **momentum** which takes the weighted average of x previous steps. Momentum is a constant beta between 0 & 1 where the previous step is 1, the one before it beta, the one before that beta^2 meaning that steps that happened a long time ago matter but not as much.\n",
    "\n",
    "### Vanishing Gradient\n",
    "\n",
    "Imagine you are utlising the sigmoid activation function. At either tail the function is very flat. This means that when you are calculating the derivate at these points that they are almost zero. This is an issue because the derivative tells us which direction to move so we'd be making very small 'steps'.\n",
    "\n",
    "To get around the above you can change the activation function to something like the **hyperbolic tangent function** which is similar to a sigmoid butwhere the range is between -1 and 1 therefore leading to larger derivatives. Another is the **rectified linear unit (ReLU)**.\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "- **Stochastic Gradient Descent:** taking small even subsets of **well distibuted** data to reduce training time _(imagine the power and time needed to do huge matrix calculations on larger networks)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
