{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "## Ensemble Methods\n",
    "##### Elliot Partridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensembles**\n",
    "Refer to how we can combine (or ensemble) the models you have already seen in a way that makes the combination of these models better at predicting than the individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Would We Want to Ensemble Learners Together?**\n",
    "There are two competing variables in finding a well fitting machine learning model: Bias and Variance.\n",
    "\n",
    "- **Bias:** When a model has high bias, this means that means it doesn't do a good job of bending to the data. An example of an algorithm that usually has high bias is linear regression. Even with completely different datasets, we end up with the same line fit to the data. When models have high bias, this is bad.\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/video.udacity-data.com/topher/2016/August/57af91bb_anscombes-quartet-3/anscombes-quartet-3.svg\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Variance:** When a model has high variance, this means that it changes drastically to meet the needs of every point in our dataset. Linear models like the one above is low variance, but high bias. An example of an algorithm that tends to have a high variance and low bias is a decision tree (especially decision trees with no early stopping parameters). A decision tree, as a high variance algorithm, will attempt to split every point into it's own branch if possible. This is a trait of high variance, low bias algorithms - they are extremely flexible to fit exactly whatever data they see.\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/video.udacity-data.com/topher/2016/December/58633876_decision-tree-sketch/decision-tree-sketch.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introducing Randomness Into Ensembles**\n",
    "Another method that is used to improve ensemble methods is to introduce randomness into high variance algorithms before they are ensembled together. The introduction of randomness combats the tendency of these algorithms to overfit (or fit directly to the data available). There are two main ways that randomness is introduced:\n",
    "\n",
    "- **Bootstrap the data** - that is, sampling the data with replacement and fitting your algorithm and fitting your algorithm to the sampled data.\n",
    "- **Subset the features** - in each split of a decision tree or with each algorithm used an ensemble only a subset of the total possible features are used.\n",
    "In fact, these are the two random components used in the next algorithm you are going to see called random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
